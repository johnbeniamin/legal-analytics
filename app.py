import streamlit as st
import pandas as pd
from collections import Counter
from nltk import ngrams
import io

# === Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØµÙØ­Ø© ===
st.set_page_config(page_title="Ø§Ù„Ù…Ø­Ù„Ù„ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ", layout="wide")

st.title("âš–ï¸ Ù…Ù†ØµØ© Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø§Ù„Ø°ÙƒÙŠ")
st.markdown("---")

# === Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠØ© (Sidebar) ===
with st.sidebar:
    st.header("Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ­Ù„ÙŠÙ„")
    uploaded_file = st.file_uploader("Ø§Ø±ÙØ¹ Ù…Ù„Ù Ø§Ù„Ù†Øµ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ (TXT)", type=['txt'])
    min_percentage = st.slider("Ø­Ø³Ø§Ø³ÙŠØ© Ø§Ù„ØªØ­Ù„ÙŠÙ„ (%)", 0.1, 2.0, 0.1)
    st.info("ÙƒÙ„ Ù…Ø§ Ø§Ù„Ø±Ù‚Ù… ÙŠÙ‚Ù„ØŒ ÙƒÙ„ Ù…Ø§ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ØªØ²ÙŠØ¯.")

# === Ù…Ù†Ø·Ù‚ Ø§Ù„ØªØ­Ù„ÙŠÙ„ (Ø§Ù„Ù…Ø§ÙƒÙŠÙ†Ø©) ===
def analyze_text(raw_text):
    # 1. Ø§Ù„ØªÙ†Ø¸ÙŠÙ
    text = raw_text.replace("ØŒ", " ").replace("(", " ").replace(")", " ")
    text = text.replace("-", " ").replace(".", " ").replace(":", " ").replace("\n", " ").replace('"', " ")
    
    stop_words = ["ÙÙŠ", "Ù…Ù†", "Ø¹Ù„Ù‰", "Ø£Ù†", "Ø£Ùˆ", "Ù‡Ø°Ø§", "Ù‡Ø°Ù‡", "ØªÙ…", "Ø§Ù„ØªÙŠ", "Ø§Ù„Ø°ÙŠ", "Ø¹Ù†", "ÙƒØ§Ù†", "Ù„Ù‡Ø§", "Ø°Ù„Ùƒ", "ÙÙ‰", "Ùˆ", "Ø¨Ù‡Ø§", "Ù„Ø§", "Ø¥Ù„Ù‰", "Ù…Ø§", "Ù…Ø¹", "ÙƒÙ„", "Ø£Ù†Ù‡", "Ù‡Ùˆ", "Ù‡ÙŠ"]
    
    all_words = text.split()
    total_count = len(all_words)
    clean_words = [w for w in all_words if w not in stop_words]
    
    results = []
    
    # 2. Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„ÙØ±Ø¯ÙŠØ©
    word_counts = Counter(clean_words)
    for word, freq in word_counts.most_common():
        pct = (freq / total_count) * 100
        if pct >= min_percentage:
            results.append({"Ø§Ù„Ø¹Ø¨Ø§Ø±Ø©": word, "Ø§Ù„ØªÙƒØ±Ø§Ø±": freq, "Ø§Ù„Ù†ÙˆØ¹": "ÙƒÙ„Ù…Ø© ÙØ±Ø¯ÙŠØ©", "Ø§Ù„Ù†Ø³Ø¨Ø©": round(pct, 2)})
            
    # 3. Ø§Ù„Ø¹Ø¨Ø§Ø±Ø§Øª Ø§Ù„Ù…Ø±ÙƒØ¨Ø©
    grams = ngrams(clean_words, 2)
    phrases = [" ".join(g) for g in grams]
    phrase_counts = Counter(phrases)
    for phrase, freq in phrase_counts.most_common():
        if freq >= 2:
            pct = (freq / total_count) * 100
            if pct >= (min_percentage / 2): # ØªØ³Ø§Ù‡Ù„ ÙÙŠ Ø§Ù„Ø¹Ø¨Ø§Ø±Ø§Øª Ø§Ù„Ù…Ø±ÙƒØ¨Ø©
                 results.append({"Ø§Ù„Ø¹Ø¨Ø§Ø±Ø©": phrase, "Ø§Ù„ØªÙƒØ±Ø§Ø±": freq, "Ø§Ù„Ù†ÙˆØ¹": "Ø¹Ø¨Ø§Ø±Ø© Ù…Ø±ÙƒØ¨Ø©", "Ø§Ù„Ù†Ø³Ø¨Ø©": round(pct, 2)})
                 
    return results, total_count

# === Ø§Ù„Ø¹Ø±Ø¶ (Frontend) ===
if uploaded_file is not None:
    # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø±ÙÙˆØ¹
    string_data = uploaded_file.read().decode("utf-8")
    
    if st.button("Ø§Ø¨Ø¯Ø£ Ø§Ù„ØªØ­Ù„ÙŠÙ„ ğŸš€"):
        with st.spinner('Ø¬Ø§Ø±ÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ...'):
            data, total = analyze_text(string_data)
            
            # Ø¹Ø±Ø¶ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø³Ø±ÙŠØ¹Ø©
            col1, col2 = st.columns(2)
            col1.metric("Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ÙƒÙ„Ù…Ø§Øª", total)
            col2.metric("Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©", len(data))
            
            # ØªØ­ÙˆÙŠÙ„ Ù„Ø¯Ø§ØªØ§ ÙØ±ÙŠÙ… ÙˆØ¹Ø±Ø¶ Ø§Ù„Ø¬Ø¯ÙˆÙ„
            df = pd.DataFrame(data)
            st.dataframe(df, use_container_width=True)
            
            # Ø²Ø±Ø§Ø± ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¥ÙƒØ³Ù„
            buffer = io.BytesIO()
            with pd.ExcelWriter(buffer, engine='xlsxwriter') as writer:
                df.to_excel(writer, index=False, sheet_name='Report')
                
            st.download_button(
                label="ğŸ“¥ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØªÙ‚Ø±ÙŠØ± (Excel)",
                data=buffer,
                file_name="legal_analysis_report.xlsx",
                mime="application/vnd.ms-excel"
            )
else:
    st.warning("Ù…Ù† ÙØ¶Ù„Ùƒ Ù‚Ù… Ø¨Ø±ÙØ¹ Ù…Ù„Ù Ù†ØµÙŠ Ù„Ù„Ø¨Ø¯Ø¡.")